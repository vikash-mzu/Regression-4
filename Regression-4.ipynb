{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6319867-f31a-4774-b461-8968cb3ac2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Lasso Regression and Its Differences\n",
    "Lasso Regression:\n",
    "•\tConcept: Lasso Regression (Least Absolute Shrinkage and Selection Operator) is a type of linear regression that includes a penalty term on the absolute values of the coefficients. The loss function for Lasso Regression is: Loss=MSE+λ∑j=1p∣βj∣\\text{Loss} = \\text{MSE} + \\lambda \\sum_{j=1}^p |\\beta_j|Loss=MSE+λ∑j=1p∣βj∣\n",
    "o\tMSE: Mean Squared Error.\n",
    "o\tλ (lambda): Regularization parameter that controls the strength of the penalty.\n",
    "Differences from Other Regression Techniques:\n",
    "•\tRegularization: Unlike ordinary least squares (OLS) regression, which only minimizes the MSE, Lasso Regression adds a penalty term that can shrink some coefficients to zero, effectively performing feature selection.\n",
    "•\tFeature Selection: Lasso can produce sparse models where some feature coefficients are exactly zero, unlike Ridge Regression which shrinks coefficients but does not set them to zero.\n",
    "•\tHandling Large Number of Features: Lasso is useful when dealing with datasets where the number of features is large relative to the number of observations.\n",
    "Q2. Main Advantage of Lasso Regression in Feature Selection\n",
    "•\tFeature Selection: The primary advantage of Lasso Regression is its ability to perform feature selection by shrinking some coefficients to exactly zero. This results in simpler, more interpretable models and helps to eliminate irrelevant or redundant features.\n",
    "Q3. Interpreting Coefficients of a Lasso Regression Model\n",
    "•\tCoefficients Interpretation: In Lasso Regression, coefficients that are not shrunk to zero represent the impact of the corresponding predictors on the response variable. Coefficients that are exactly zero imply that the associated features have been excluded from the model. The magnitude of the non-zero coefficients reflects the strength of the relationship between the predictors and the response variable, but with the regularization effect reducing their size.\n",
    "Q4. Tuning Parameters in Lasso Regression\n",
    "•\tRegularization Parameter (λ): The key tuning parameter in Lasso Regression is λ (lambda), which controls the strength of the penalty.\n",
    "o\tHigh λ: Results in more coefficients being shrunk to zero, increasing sparsity and feature selection but potentially leading to underfitting.\n",
    "o\tLow λ: Results in less shrinkage and retains more coefficients, which may lead to overfitting if there are many irrelevant features.\n",
    "•\tChoosing λ: Use techniques such as cross-validation to find the optimal value of λ that balances bias and variance.\n",
    "Q5. Lasso Regression for Non-Linear Regression Problems\n",
    "•\tNon-Linear Problems: Lasso Regression is inherently a linear model and is designed for linear relationships between features and the target variable. However, you can handle non-linear relationships by transforming features using polynomial features or other non-linear transformations before applying Lasso Regression. This way, Lasso can help with feature selection even in transformed feature spaces.\n",
    "Q6. Difference Between Ridge Regression and Lasso Regression\n",
    "•\tPenalty Term:\n",
    "o\tRidge Regression: Uses an L2 penalty (squared magnitude of coefficients): Loss=MSE+λ∑j=1pβj2\\text{Loss} = \\text{MSE} + \\lambda \\sum_{j=1}^p \\beta_j^2Loss=MSE+λ∑j=1pβj2\n",
    "o\tLasso Regression: Uses an L1 penalty (absolute value of coefficients): Loss=MSE+λ∑j=1p∣βj∣\\text{Loss} = \\text{MSE} + \\lambda \\sum_{j=1}^p |\\beta_j|Loss=MSE+λ∑j=1p∣βj∣\n",
    "•\tFeature Selection:\n",
    "o\tRidge Regression: Shrinks coefficients but does not set any to zero. All features are included in the model.\n",
    "o\tLasso Regression: Shrinks some coefficients to zero, performing feature selection by excluding certain features.\n",
    "Q7. Handling Multicollinearity in Lasso Regression\n",
    "•\tMulticollinearity: Lasso Regression can help handle multicollinearity by shrinking the coefficients of correlated predictors and potentially setting some to zero. This results in a more stable and interpretable model when features are highly correlated.\n",
    "Q8. Choosing the Optimal Value of Regularization Parameter (λ) in Lasso Regression\n",
    "•\tCross-Validation: Use k-fold cross-validation to evaluate model performance for different values of λ. Choose the λ that provides the best trade-off between bias and variance based on the cross-validation results.\n",
    "•\tGrid Search: Perform a grid search over a range of λ values to find the optimal one.\n",
    "•\tRegularization Path Algorithms: Utilize algorithms like LARS (Least Angle Regression) to efficiently compute solutions for a range of λ values.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
